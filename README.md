ğŸš€ GenAI RAG Assistant

A Retrieval-Augmented Generation (RAG) powered AI assistant built using FastAPI, Vector Search, and LLM integration.

This system retrieves relevant document chunks using embeddings and generates context-aware responses using a Large Language Model.

ğŸ— Architecture Diagram
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      User (UI)      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     FastAPI API     â”‚
                â”‚   (/api/chat)       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Embedding Model    â”‚
                â”‚ (Query â†’ Vector)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Vector Store (DB)  â”‚
                â”‚ Similarity Search   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Retrieved Chunks   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     LLM Model       â”‚
                â”‚  (Prompt + Context) â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   Generated Answer  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ğŸ”„ RAG Workflow Explanation

The system follows the Retrieval-Augmented Generation pipeline:

Step 1 â€” User Query

The user sends a message to /api/chat.

Step 2 â€” Query Embedding

The user message is converted into a vector embedding using an embedding model.

Step 3 â€” Similarity Search

The embedding is compared against stored document embeddings in the vector database using cosine similarity.

Step 4 â€” Context Retrieval

Top-K most relevant document chunks are retrieved.

Step 5 â€” Prompt Construction

The retrieved chunks are injected into a structured prompt template.

Step 6 â€” LLM Generation

The prompt is passed to the LLM, which generates a grounded response based on retrieved context.

Step 7 â€” Response Return

The API returns:

reply

tokensUsed

retrievedChunks

ğŸ§  Embedding Strategy
Model Used

A transformer-based embedding model is used to convert both:

Documents

User queries

into dense vector representations.

Why Embeddings?

Embeddings allow semantic search instead of keyword matching.

Example:

User asks:

"How do I recover my password?"

System can match:

"Reset your password via Security Settings"

Even if wording is different.

Chunking Strategy

Documents are:

Split into fixed-size chunks

Overlap added (optional) to preserve context continuity

This improves retrieval accuracy.

ğŸ” Similarity Search Explanation

The system uses vector similarity (typically cosine similarity).

Cosine Similarity Formula
similarity = (A Â· B) / (||A|| ||B||)

Where:

A = Query embedding

B = Document embedding

Higher score â†’ More semantically similar.

Why Top-K Retrieval?

Instead of retrieving a single document:

Top 3â€“5 chunks are retrieved

Improves answer grounding

Reduces hallucination

ğŸ§© Prompt Design Reasoning

Prompt structure:

You are a helpful assistant.
Use the provided context to answer the question.
If the answer is not in the context, say you don't know.

Context:
{retrieved_chunks}

Question:
{user_question}
Why This Structure?

Prevents hallucinations

Forces grounding in retrieved documents

Encourages safe fallback behavior

Makes output deterministic and reliable

ğŸ›  Tech Stack

Python

FastAPI

Uvicorn

Embedding Model (e.g. OpenAI / HuggingFace)

Vector Store (e.g. FAISS / ChromaDB)

LLM (OpenAI / Local model)

âš™ï¸ Setup Instructions
1ï¸âƒ£ Clone Repository
git clone <your-repo-url>
cd genai-rag-assistant
2ï¸âƒ£ Create Virtual Environment
python -m venv venv

Activate:

Windows:
venv\Scripts\activate
Mac/Linux:
source venv/bin/activate
3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
4ï¸âƒ£ Set Environment Variables

Create .env file:

OPENAI_API_KEY=your_api_key_here
5ï¸âƒ£ Run Backend
uvicorn app:app --reload

Open:

http://127.0.0.1:8000/docs
6ï¸âƒ£ Test API

Use Swagger UI:

POST â†’ /api/chat

Example Request:

{
  "sessionId": "123",
  "message": "How can I reset my password?"
}
ğŸ“Š Example Response
{
  "reply": "You can reset your password from Settings > Security...",
  "tokensUsed": 101,
  "retrievedChunks": 1
}
